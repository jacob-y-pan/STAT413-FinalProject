---
title: "Final Project"
output: pdf_document
date: "December 12, 2023"
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(ISLR2)
library(isotree)
library(ellipse)

set.seed(1)
```


Real World Data:
```{r}
# Load the Auto data from ISLR package
auto_df = Auto[, 1:6] # Only use the numeric data

# Plot the pairwise data relationships
plot(auto_df, pch = 20, main = "Auto Data Pairwise Relationships")
```
Many of the variables seem to be correlated, with some linear and some polynomial relationships. Looking at just the pairwise relationships, there are obviously at least a few outliers in the data. We can use Mahalanobis distance as a benchmark to detect outliers in the data. This will be used to compare our other methods' results.

```{r}
# Test if the numeric data has any outliers according to Mahalanobis distance
auto_center = colMeans(auto_df)
auto_cov = cov(auto_df)
auto_dst = mahalanobis(auto_df, auto_center, auto_cov)
auto_cutoff = qchisq(0.95, df = ncol(auto_df))

nrow(auto_df[auto_dst > auto_cutoff, ]) # Number of observations considered outliers
```

Using Mahalanobis distance, we can see that there are 38 observations in the Auto dataset that are considered outliers. We can visualize this by plotting the data and coloring the points based on their Mahalanobis distance. We can also plot the Mahalanobis distance ellipses for horsepower vs mpg and acceleration vs mpg, which gives us a reference for the outlier threshold if we were using just the two variables.

```{r}
# Create a scatter plot of horsepower vs mpg, coloring based on Mahalanobis distance
hp_ellipse = as.data.frame(ellipse(cor(auto_df[, c(4, 1)]), scale = c(sd(auto_df[,4]), sd(auto_df[,1])), centre = c(auto_center[4], auto_center[1])), level = 0.95)

ggplot(auto_df, aes(x = horsepower, y = mpg)) + 
  geom_point(aes(color=auto_dst)) + 
  geom_polygon(data = hp_ellipse, alpha = 0.2, fill = "blue", color = "blue") +
  labs(title = "Visualizing Mahalanobis Distance for Horsepower vs. MPG", x = "Horsepower", y = "MPG", color="Mahalanobis Distance") +
  scale_color_gradient(low = "yellow", high = "red") +
  theme_minimal()
```

```{r}
# Create a scatter plot of acceleration vs mpg, coloring based on Mahalanobis distance
acc_ellipse = as.data.frame(ellipse(cor(auto_df[, c(6, 1)]), scale = c(sd(auto_df[,6]), sd(auto_df[,1])), centre = c(auto_center[6], auto_center[1])), level = 0.95)

ggplot(auto_df, aes(x = acceleration, y = mpg)) + 
  geom_point(aes(color=auto_dst)) + 
  geom_polygon(data = acc_ellipse, alpha = 0.2, fill = "blue", color = "blue") +
  labs(title = "Visualizing Mahalanobis Distance for Acceleration vs. MPG", x = "Acceleration", y = "MPG", color="Mahalanobis Distance") +
  scale_color_gradient(low = "yellow", high = "red") +
  theme_minimal()
```

```{r}
# Perform isolation forest on the auto data
if_model = isolation.forest(auto_df[, 1:6], ntrees = 500, nthreads = 1)
if_pred = predict(if_model, auto_df[, 1:6])
if_outliers = as.factor(ifelse(if_pred > 0.5, "Outlier", "Not Outlier"))

length(if_outliers[if_outliers == "Outlier"]) # Number of outliers based on isolation forest
```
There are 49 outliers based on the isolation forest model. We can visualize this by plotting the data and coloring the points based on if the isolation forest predicted them as an outlier. We can also plot the Mahalanobis distance ellipses, which allows us to compare the isolation forest results to the Mahalanobis distance results.

```{r}
# Plot the isolation forest outliers
ggplot(auto_df, aes(x = horsepower, y = mpg, color = if_outliers)) + 
  geom_point() + 
  geom_polygon(data = hp_ellipse, alpha = 0.2, color = "blue", fill = "blue") +
  labs(title = "Visualizing Isolation Forest on Horsepower vs. MPG", x = "Acceleration", y = "MPG", color = "Isolation Forest Result", caption = "\nBlue ellipse represents Mahalanobis distance outlier threshold") +
  theme_minimal()
```

```{r}
ggplot(auto_df, aes(x = acceleration, y = mpg, color = if_outliers)) + 
  geom_point() + 
  geom_polygon(data = acc_ellipse, alpha = 0.2, fill = "blue", color = "blue") +
  labs(title = "Visualizing Isolation Forest on Acceleration vs. MPG", x = "Acceleration", y = "MPG", color="Isolation Forest Result", caption = "\nBlue ellipse represents Mahalanobis distance outlier threshold") +
  theme_minimal()
```

The results of using Isolation Forest is very similar to using Mahalanobis distance, with the exception of a few points. The Mahalanobis distance seems to be more conservative than the isolation forest results. This is likely due to the fact that isolation forest is based on the number of splits it takes to isolate a point, which is able to measure non-linear outliers between variables.



To measure the performance of the outlier detection methods, we will use the R^2 value of a linear regression model using mpg as the response variable. We will use the R^2 value of the linear regression model with all of the data, and then the R^2 value of the linear regression model with the outliers removed. This will mimic the methods' ability to detect bad data points to improve data/model quality.

```{r}
# Create a linear regression model using all of the data
original_lm = lm(mpg ~ ., data = auto_df)
summary(original_lm)$r.squared # R^2 value of the original linear regression model
```

```{r}
# Model with outliers removed using Mahalanobis distance
mahal_lm = lm(mpg ~ ., data = auto_df[auto_dst <= auto_cutoff, ])
summary(mahal_lm)$r.squared # R^2 value of the original linear regression model
```

```{r}
# Model with outliers removed using isolation forest
if_lm = lm(mpg ~ ., data = auto_df[if_outliers == "Not Outlier", ])
summary(if_lm)$r.squared # R^2 value of the original linear regression model
```




```{r A}
# This code is to generate simulated data with outliers
# Generate a polynomial underlying, with random noise
x1poly = runif(100, min = -1, max = 1)

ypoly = 2*x1poly^2 + 3*x1poly + 4 + rnorm(100, mean = 0, sd = 1)
plot(x1poly, ypoly, main = "Simulated Data", xlab = "x1", ylab = "y")

# Add outliers
outlierspoly = c(-0.4, 0, 0.2, 0.5, 0.8)
x1poly = c(x1poly, outlierspoly)
# outlieryvalues = c(1, -1, 1, -1, 1)
ypoly = c(ypoly, mean(y))
ypoly
# Make only the outliers red 
plot(x1poly, ypoly, main = "Simulated Data with Outliers", xlab = "x1", ylab = "y", col = ifelse(x1poly %in% outlierspoly, "red", "black"))

# Calculate mahalabonis distance of each point to confirm outliers are outliers
mahalabonispoly = mahalanobis(x = cbind(x1poly, ypoly), center = colMeans(cbind(x1poly, ypoly)), cov = cov(cbind(x1poly, ypoly)))
mahalabonispoly
# Find p-value of each point
pvaluespoly = 1 - pchisq(mahalabonispoly, df = 2)
# Any points with p-value < 0.01 are outliers, detect them
outlierspoly = x1poly[pvaluespoly < 0.01]
outlierspoly
```

```{r B}
# Same procedure, except with a mutlivariable underlying
set.seed(1)
x1complex = runif(100, min = -1, max = 1)
x2complex = runif(100, min = -1, max = 1)

ycomplex = 2*x1complex^2 + 3*x1complex + 4 + 2*x2complex^2 + 3*x2complex + 4 + rnorm(100, mean = 0, sd = 1)

plot(x1complex, ycomplex, main = "Simulated Data", xlab = "x1", ylab = "y")
plot(x2complex, ycomplex, main = "Simulated Data", xlab = "x2", ylab = "y")

outlierscomplexx1 = c(-0.4, 0, 0.2, 0.5)
# Different x points for x2
outlierscomplexx2 = c(-0.9, -0.5, 0.25, 0.8)
x1complex = c(x1complex, outlierscomplexx1)
x2complex = c(x2complex, outlierscomplexx2)
ycomplex = c(ycomplex, 20, 20, 2, 1)

plot(x1complex, ycomplex, main = "Simulated Data with Outliers", xlab = "x1", ylab = "y", col = ifelse(x1complex %in% outlierscomplexx1, "red", "black"))
plot(x2complex, ycomplex, main = "Simulated Data with Outliers", xlab = "x2", ylab = "y", col = ifelse(x2complex %in% outlierscomplexx2, "red", "black"))

maha1 = mahalanobis(x = cbind(x1complex, ycomplex), center = colMeans(cbind(x1complex, ycomplex)), cov = cov(cbind(x1complex, ycomplex)))
maha2 = mahalanobis(x = cbind(x2complex, ycomplex), center = colMeans(cbind(x2complex, ycomplex)), cov = cov(cbind(x2complex, ycomplex)))

pvalues1 = 1 - pchisq(maha1, df = 2)
pvalues2 = 1 - pchisq(maha2, df = 2)

outlierscomplexx1 = x1complex[pvalues1 < 0.01]
outlierscomplexx2 = x2complex[pvalues2 < 0.01]
# Table with outliers of x1 and x2 in different columns
outlierscomplex = data.frame(outlierscomplexx1, outlierscomplexx2)
outlierscomplex
```

```{r C}
```

```{r C}
```

```{r C}
```

```{r C}
```
>>>>>>> 394740410c33d53dfce5e21c83e65429dba2d748
